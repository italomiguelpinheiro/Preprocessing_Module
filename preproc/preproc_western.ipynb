{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cd90dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install nltk\n",
    "# ! pip install spellchecker\n",
    "# ! pip install autocorrect\n",
    "# ! pip install presidio_analyzer\n",
    "# ! pip install presidio_anonymizer\n",
    "# ! pip install spacy\n",
    "# ! pip install pyspellchecker\n",
    "# ! python3 -m spacy download en_core_web_sm\n",
    "# ! python3 -m spacy download es_core_news_sm\n",
    "# ! python3 -m spacy download fr_core_news_sm\n",
    "# ! python3 -m spacy download en_core_web_lg\n",
    "# ! python3 -m spacy download pt_core_news_sm\n",
    "# ! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "810d8d6c-9180-482a-b57e-0b9dfc5dbccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nicolas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/nicolas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Important: Adjust data reading to your needs\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "from autocorrect import Speller\n",
    "\n",
    "from presidio_analyzer import AnalyzerEngine\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from presidio_analyzer import PatternRecognizer\n",
    "from presidio_analyzer import RecognizerRegistry\n",
    "from presidio_analyzer.nlp_engine import NlpEngineProvider\n",
    "\n",
    "import re\n",
    "import os\n",
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "from sklearn.preprocessing import OneHotEncoder as OHE\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "punctuation = [p for p in punctuation ]\n",
    "punctuation.remove('?')\n",
    "punctuation.remove('!')\n",
    "punctuation.remove('\"')\n",
    "import spacy\n",
    "print(\" Important: Adjust data reading to your needs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27cfe87a-015b-46cd-a112-c10a7214e01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_pt = spacy.load(\"pt_core_news_sm\")\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "nlp_es = spacy.load(\"es_core_news_sm\")\n",
    "nlp_fr = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d0af74b-64a4-4509-aa22-03a05951df8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_and_texts(values, n_text, n_label):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for e in values:\n",
    "        utt = e[n_text]\n",
    "        texts.append(utt)\n",
    "    for e in values:\n",
    "        utt = e[n_label]\n",
    "        labels.append(utt)\n",
    "    #print(texts, labels)\n",
    "    data = pd.DataFrame(data = {\"text\": texts, \"label\": labels})\n",
    "\n",
    "    return data\n",
    "\n",
    "def read_df(data, index_text, index_label):\n",
    "    values = data.values\n",
    "    df = get_labels_and_texts(values, index_text, index_label)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "915a6b3e-5eda-491a-8cac-01c2dddec9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupby(df, column):\n",
    "    print(str(df.groupby(column).count()))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6000366-0a2b-42d8-96cb-260212fa387d",
   "metadata": {},
   "source": [
    "# Target One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d2ed3dd-666e-4c88-a7da-1326705bb28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding_func(df):\n",
    "    y_encoder= OHE().fit(np.array(df.label).reshape(-1,1))\n",
    "    ytr_encoded= y_encoder.transform(np.array(df.label).reshape(-1,1)).toarray()\n",
    "    \n",
    "    return ytr_encoded\n",
    "#print(one_hot_encoding(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239a9c79-b844-4c87-9b70-c580a03db0dc",
   "metadata": {},
   "source": [
    "# Convert text to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f744a27-97a8-4ec0-a101-e0b5a02892b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_lowercase_func(df):\n",
    "    df[\"text\"]= df.text.map(lambda x: x.lower())\n",
    "    return df\n",
    "#print(convert_to_lowercase(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2d6134-e99e-49b8-8705-90bb5ec2318f",
   "metadata": {},
   "source": [
    "# Data Protection and Anonymization (presidio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b882625b-bc62-42d9-a4e7-94a83ba98344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the engine, loads the NLP module (spaCy model by default) \n",
    "# and other PII recognizers\n",
    "\n",
    "def remove_synonyms_email(text):\n",
    "    # deve vir depois da mascara de phone_number\n",
    "    text = re.sub('e-mails|e-mail|emails|emailed|email|mail', 'email', text, flags=re.I)\n",
    "    return text\n",
    "    \n",
    "\n",
    "def anonymize_link(text):\n",
    "    regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "    text = re.sub(regex, '<LINK>', text)\n",
    "    text = re.sub('dell.com', '<LINK>', text)\n",
    "    return text\n",
    "            \n",
    "def anonymize_date(text):\n",
    "    text = re.sub(r\"[\\d]{1,2}/[\\d]{1,2}/[\\d]{4}\", '<DATE>', text) #10/10/2015\n",
    "    text = re.sub(r\"[\\d]{1,2}-[\\d]{1,2}-[\\d]{2}\", '<DATE>', text) #10-10-15\n",
    "    text = re.sub(r\"[\\d]{1,2} [ADFJMNOS]\\w* [\\d]{4}\", '<DATE>',text) #1 NOV 2010\n",
    "    text = re.sub(r\"[\\d]{1,2} [ADFJMNOS]\\w* [\\d]{4}\", '<DATE>', text) #10 March 2015\n",
    "    return text\n",
    "\n",
    "def anonymize_phone_number(text):\n",
    "    if re.findall(r'téléphone|teléfono|telefone|phone', text.lower()):\n",
    "        text = re.sub(r'0-000-000-0000', 'PHONE_NUMBER', text)\n",
    "        text = re.sub(r'\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4}', 'PHONE_NUMBER', text)\n",
    "    return text\n",
    "\n",
    "def anonymize_service_request(text):\n",
    "    # deve vir depois da mascara de phone_number\n",
    "    if re.findall(r'servidor|servicio|serveur|service|request|sr', text, flags=re.I):\n",
    "        text = re.sub(r'\\d{5,}', '<SR_NUMBER>', text)\n",
    "    return text\n",
    "\n",
    "def anonymize_number(text):\n",
    "    # deve vir depois da mascara de phone_number\n",
    "    text = re.sub(r'\\d{5,}', '<NUMBER>', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "919c626d-84ac-4720-89da-3a5442d35e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = {\n",
    "    \"nlp_engine_name\": \"spacy\",\n",
    "    \"models\": [{\"lang_code\": \"es\", \"model_name\": \"es_core_news_sm\"},\n",
    "               {\"lang_code\": \"en\", \"model_name\": \"en_core_web_lg\"},\n",
    "               {\"lang_code\": \"pt\", \"model_name\": \"pt_core_news_sm\"},\n",
    "               {\"lang_code\": \"fr\", \"model_name\": \"fr_core_news_sm\"}\n",
    "              ],\n",
    "}\n",
    "\n",
    "anonymizer = AnonymizerEngine()\n",
    "provider = NlpEngineProvider(nlp_configuration=configuration)\n",
    "nlp_engine_with_multilanguages = provider.create_engine()\n",
    "analyzer = AnalyzerEngine( nlp_engine=nlp_engine_with_multilanguages, \n",
    "    supported_languages=[\"en\", \"es\", \"pt\", \"fr\"])\n",
    "\n",
    "\n",
    "def presidio(language, text):\n",
    "    results = analyzer.analyze(text=text,\n",
    "                               entities=[\"PHONE_NUMBER\", \"EMAIL_ADDRESS\" ],\n",
    "                               language=language)\n",
    "\n",
    "    anonymized_text = anonymizer.anonymize(text=text,analyzer_results=results)\n",
    "    text = anonymize_link(anonymized_text.text)\n",
    "    text = anonymize_service_request(text)\n",
    "    text = remove_synonyms_email(text)\n",
    "    text = anonymize_date(text)\n",
    "    text = anonymize_phone_number(text)\n",
    "    text = anonymize_number(text)\n",
    "    \n",
    "    #print(text)\n",
    "    return text\n",
    "\n",
    "def presidio_func(df, lang):\n",
    "    df[\"text\"]= df.text.progress_map(lambda x: presidio(language=lang, text=x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdf064d-d579-4d4e-8689-587dacb5fd50",
   "metadata": {},
   "source": [
    "# Word Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0642b86-2650-47d2-9eae-b57618b550f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    return \" \".join(text)\n",
    "\n",
    "def word_tokenize_func(df, tokenize):\n",
    "    df[\"text\"]= df.text.progress_map(tokenize)\n",
    "    df[\"text\"]= df.text.apply(normalize)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed37533f-e086-4612-9412-5cf6f9c6c45c",
   "metadata": {},
   "source": [
    "# SpellCheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef4ff044-83c4-4c28-a456-da61283abe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    return \" \".join(text)\n",
    "\n",
    "def spell_checker_func(df, lang):\n",
    "    spell = Speller(fast=True, lang=lang)\n",
    "    df[\"text\"] = df.text.progress_map(lambda xs: [spell(x) for x in xs.split()])\n",
    "    df[\"text\"]= df.text.apply(normalize)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93540e7-9c25-4b88-98cc-63fc7aeda30b",
   "metadata": {},
   "source": [
    "# Remove StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c91abd98-68bb-4a2b-b56b-89e29279ea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Stop Words\n",
    "def remove_stop(strings, stop_list):\n",
    "    classed= [s for s in strings.split() if s not in stop_list]\n",
    "    return classed\n",
    "def normalize(text):\n",
    "    return \" \".join(text)\n",
    "\n",
    "def remove_stopwords_func(df,language):\n",
    "    stop= stopwords.words(language)\n",
    "    stop_punc= list(set(punctuation))+ stop\n",
    "    df[\"text\"]= df.text.progress_map(lambda x: remove_stop(x, stop_punc))\n",
    "    df[\"text\"]= df.text.apply(normalize)\n",
    "    return df\n",
    "\n",
    "#remove_stopwords(df)\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f80b3cd-0d5c-4d47-bb40-27b6efaf355f",
   "metadata": {},
   "source": [
    "# Lemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b0f940a-c46f-4942-84c0-e809dec57db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemme(text, lang):\n",
    "    if lang == \"english\":\n",
    "        doc = nlp_en(text)\n",
    "    elif lang == \"spanish\":\n",
    "        doc = nlp_es(text)\n",
    "    elif lang == \"portuguese\":\n",
    "        doc = nlp_pt(text)\n",
    "    elif lang == \"french\": \n",
    "        doc = nlp_fr(text)\n",
    "    phrase = \" \".join([token.lemma_ for token in doc])\n",
    "    #print(phrase)\n",
    "    return phrase\n",
    "\n",
    "def lemming_func(df, lang):\n",
    "    df[\"text\"]= df.text.progress_map(lambda text: lemme(text, lang))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfce2892-5cee-429c-a8d0-c9df9b690657",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "795a2d2f-1e2a-40b8-b37b-d59c92ad8843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    return \" \".join(text)\n",
    "\n",
    "def stemming_func(df):\n",
    "    stemmer= PorterStemmer()\n",
    "    df[\"text\"]= df.text.progress_map(lambda xs: [stemmer.stem(x) for x in xs.split()])\n",
    "    df[\"text\"]= df.text.apply(normalize)\n",
    "    \n",
    "    return df\n",
    "\n",
    "#stemming(df)\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa26fd18-af3d-4f7b-af7a-42d34d40a88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data,language, hot_encoding = False, lower_case= False, presidio=False, \n",
    "               word_tokenize= False, w_tokenize = word_tokenize, spell_checker= False, \n",
    "               remove_stopwords = False, lemming=False, steeming= False, replace_syn=False):\n",
    "    \n",
    "    encoding = hot_encoding\n",
    "    lower = lower_case\n",
    "    pred = presidio\n",
    "    tokenize =  word_tokenize\n",
    "    stopwords = remove_stopwords\n",
    "    stee = steeming\n",
    "    lemme = lemming\n",
    "    spell = spell_checker\n",
    "    \n",
    "    df = groupby(data, \"label\")\n",
    "    \n",
    "    \n",
    "    if language == \"portuguese\":\n",
    "        lang = 'pt'\n",
    "    elif language == \"english\":\n",
    "        lang = 'en'\n",
    "    elif language == \"spanish\": \n",
    "        lang = 'es'\n",
    "    elif language == \"french\":\n",
    "        lang = 'fr'\n",
    "    \n",
    "    \n",
    "    if encoding == True:\n",
    "        print(\"One Hot Encoding:\")\n",
    "        encoded = one_hot_encoding_func(df)\n",
    "        print(encoded)\n",
    "        print(\"...\")\n",
    "    if lower == True:\n",
    "        print(\"Converting to LowerCase\")\n",
    "        df = convert_to_lowercase_func(df)\n",
    "    if pred == True:\n",
    "        print(\"Doing Data Protection and Anonymization\")\n",
    "        df = presidio_func(df,lang)\n",
    "    if tokenize ==  True:\n",
    "        print(\"Doing word_tokenize\")\n",
    "        df = word_tokenize_func(df, w_tokenize)\n",
    "    if spell == True: \n",
    "        print(\"Doing Spell Checking\")\n",
    "        df = spell_checker_func(df, lang)\n",
    "    if stopwords == True:\n",
    "        print(\"Removing StopWords\")\n",
    "        df = remove_stopwords_func(df,language)\n",
    "    if lemme == True:\n",
    "        print(\"Doing Lemmatization\")\n",
    "        df = lemming_func(df, language)\n",
    "    if stee == True:\n",
    "        print(\"Doing steeming\")\n",
    "        df = stemming_func(df)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d72db6-b7c1-4ac5-979b-4b73a49dc01c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf76f01-83f0-49d5-b1d0-fb477e2af4c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
